{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms, functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'K', 'M',\n",
    "         'N', 'P', 'R', 'T', 'U', 'V', 'W', 'X', 'Y']\n",
    "\n",
    "ONE_HOT = torch.eye(len(CHARS))\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder, img_list, transform=None):\n",
    "        self.folder = folder\n",
    "        self.im_list = img_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.im_list[idx][:4]\n",
    "        path = os.path.join(self.folder, self.im_list[idx])\n",
    "        im = Image.open(path)\n",
    "        if im.mode != 'RGB':\n",
    "            im = im.convert('RGB')\n",
    "        sample = {'image': im, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2OneHot(object):\n",
    "    def __call__(self, sample):\n",
    "        labels = list()\n",
    "        for c in sample['label']:\n",
    "            idx = CHARS.index(c)\n",
    "            labels.append(ONE_HOT[idx])\n",
    "        sample['label'] = torch.cat(labels)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        np_img = np.asarray(sample['image'])\n",
    "        image = np_img.transpose((2, 0, 1))  # H x W x C  -->  C x H x W\n",
    "        sample['image'] = torch.from_numpy(image).float()\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(transforms.Normalize):\n",
    "    def __call__(self, sample):\n",
    "        tensor = sample['image']\n",
    "        sample['image'] = functional.normalize(\n",
    "            tensor, self.mean, self.std, self.inplace)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToGPU(object):\n",
    "    def __call__(self, sample):\n",
    "        sample['image'] = sample['image'].to(DEVICE)\n",
    "        sample['label'] = sample['label'].float().to(DEVICE)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=4, max_m=-1, split_rate=0.2, gpu=True):\n",
    "    # list images\n",
    "    wd, _ = os.path.split(os.path.abspath(__file__))\n",
    "    folder = os.path.join(wd, 'data')\n",
    "    imgs = [i for i in os.listdir(folder) if i.endswith('jpg')]\n",
    "    if not imgs:\n",
    "        raise Exception('Empty folder!')\n",
    "    random.seed(1)\n",
    "    random.shuffle(imgs)\n",
    "    point = int(split_rate * len(imgs))\n",
    "    train_imgs = imgs[point:][:max_m]\n",
    "    valid_imgs = imgs[:point][:max_m]\n",
    "\n",
    "    # initialize transform\n",
    "    chains = [Word2OneHot(),\n",
    "              ImgToTensor(),\n",
    "              Normalize([127.5, 127.5, 127.5], [128, 128, 128])]\n",
    "    if gpu:\n",
    "        chains.append(ToGPU())\n",
    "    transform = transforms.Compose(chains)\n",
    "\n",
    "    # load data\n",
    "    train_ds = ImageDataset(folder, train_imgs, transform=transform)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    valid_ds = ImageDataset(folder, valid_imgs, transform=transform)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size)\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img * 128 + 127.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    im = Image.fromarray(npimg.astype('uint8'))\n",
    "    im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_time(start, end):\n",
    "    s = int(end-start)\n",
    "    if s < 60:\n",
    "        return '{}s'.format(s)\n",
    "    m = s // 60\n",
    "    s = s % 60\n",
    "    if m < 59:\n",
    "        return '{}m {}s'.format(m, s)\n",
    "    h = m // 60\n",
    "    m = m % 60\n",
    "    return '{}h {}m {}s'.format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "from timeit import default_timer as timer\n",
    "from utils import load_data, DEVICE, human_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, gpu=False):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 18, 5)  # 18 * 32 * 116\n",
    "        self.pool1 = nn.MaxPool2d(2)  # 18 * 16 * 58\n",
    "        self.conv2 = nn.Conv2d(18, 48, 5)  # 48 * 12 * 54\n",
    "        self.pool2 = nn.MaxPool2d(2)  # 48 * 6 * 27\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(48 * 6 * 27, 360)\n",
    "        self.fc2 = nn.Linear(360, 19 * 4)\n",
    "\n",
    "        if gpu:\n",
    "            self.to(DEVICE)\n",
    "            if str(DEVICE) == 'cpu':\n",
    "                self.device = 'cpu'\n",
    "            else:\n",
    "                self.device = torch.cuda.get_device_name(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 48 * 6 * 27)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x).view(-1, 4, 19)\n",
    "        x = F.softmax(x, dim=2)\n",
    "        x = x.view(-1, 4 * 19)\n",
    "        return x\n",
    "\n",
    "    def save(self, name, folder='./models'):\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        path = os.path.join(folder, name)\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, name, folder='./models'):\n",
    "        path = os.path.join(folder, name)\n",
    "        map_location = 'cpu' if self.device == 'cpu' else 'gpu'\n",
    "        static_dict = torch.load(path, map_location)\n",
    "        self.load_state_dict(static_dict)\n",
    "        self.eval()\n",
    "\n",
    "    def graph(self):\n",
    "        x = torch.rand(1, 3, 40, 150)\n",
    "        y = self(x)\n",
    "        return make_dot(y, params=dict(self.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, data, opt=None):\n",
    "    xb, yb = data['image'], data['label']\n",
    "    batch_size = len(xb)\n",
    "    out = model(xb)\n",
    "    loss = loss_func(out, yb)\n",
    "\n",
    "    single_correct, whole_correct = 0, 0\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    else:  # calc accuracy\n",
    "        yb = yb.view(-1, 4, 19)\n",
    "        out_matrix = out.view(-1, 4, 19)\n",
    "        _, ans = torch.max(yb, 2)\n",
    "        _, predicted = torch.max(out_matrix, 2)\n",
    "        compare = (predicted == ans)\n",
    "        single_correct = compare.sum().item()\n",
    "        for i in range(batch_size):\n",
    "            if compare[i].sum().item() == 4:\n",
    "                whole_correct += 1\n",
    "        del out_matrix\n",
    "    loss_item = loss.item()\n",
    "    del out\n",
    "    del loss\n",
    "    return loss_item, single_correct, whole_correct, batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl, verbose=None):\n",
    "    max_acc = 0\n",
    "    patience_limit = 5\n",
    "    patience = 0\n",
    "    for epoch in range(epochs):\n",
    "        patience += 1\n",
    "        running_loss = 0.0\n",
    "        total_nums = 0\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_dl):\n",
    "            loss, _, _, s = loss_batch(model, loss_func, data, opt)\n",
    "            if isinstance(verbose, int):\n",
    "                running_loss += loss * s\n",
    "                total_nums += s\n",
    "                if i % verbose == verbose - 1:\n",
    "                    ave_loss = running_loss / total_nums\n",
    "                    print('[Epoch {}][Batch {}] got training loss: {:.6f}'\n",
    "                          .format(epoch + 1, i + 1, ave_loss))\n",
    "                    total_nums = 0\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        model.eval()  # validate model, working for drop out layer.\n",
    "        with torch.no_grad():\n",
    "            losses, single, whole, batch_size = zip(\n",
    "                *[loss_batch(model, loss_func, data) for data in valid_dl]\n",
    "            )\n",
    "        total_size = np.sum(batch_size)\n",
    "        val_loss = np.sum(np.multiply(losses, batch_size)) / total_size\n",
    "        single_rate = 100 * np.sum(single) / (total_size * 4)\n",
    "        whole_rate = 100 * np.sum(whole) / total_size\n",
    "        if single_rate > max_acc:\n",
    "            patience = 0\n",
    "            max_acc = single_rate\n",
    "            model.save('pretrained')\n",
    "\n",
    "        print('After epoch {}: \\n'\n",
    "              '\\tLoss: {:.6f}\\n'\n",
    "              '\\tSingle Acc: {:.2f}%\\n'\n",
    "              '\\tWhole Acc: {:.2f}%'\n",
    "              .format(epoch + 1, val_loss, single_rate, whole_rate))\n",
    "        if patience > patience_limit:\n",
    "            print('Early stop at epoch {}'.format(epoch + 1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(use_gpu=True, epochs=40, verbose=500):\n",
    "    train_dl, valid_dl = load_data(batch_size=4, split_rate=0.2, gpu=use_gpu)\n",
    "    model = Net(use_gpu)\n",
    "    opt = optim.Adadelta(model.parameters())\n",
    "    criterion = nn.BCELoss()\n",
    "    start = timer()\n",
    "    fit(epochs, model, criterion, opt, train_dl, valid_dl, verbose)\n",
    "    end = timer()\n",
    "    t = human_time(start, end)\n",
    "    print('Total training time using {}: {}'.format(model.device, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-19b5ffe7f3df>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(use_gpu, epochs, verbose)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhuman_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-c4624c5ab0cb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, train_dl, valid_dl, verbose)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-16aa641a22ce>\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, loss_func, data, opt)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# calc accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/oedipus/lib/python3.7/site-packages/torch/optim/adadelta.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
