{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oedipus Notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidpierre21/oedipus/blob/master/Oedipus%20Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GaZBB6l1aAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import transforms, functional"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJvNeFPD1utH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "91674bab-f195-4a09-c9aa-cc719c20c4b8"
      },
      "source": [
        "!git clone https://gitlab.com/davidpierrea/datasets1.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'datasets1'...\n",
            "remote: Enumerating objects: 15006, done.\u001b[K\n",
            "remote: Counting objects: 100% (15006/15006), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15005/15005), done.\u001b[K\n",
            "remote: Total 15006 (delta 0), reused 15006 (delta 0)\u001b[K\n",
            "Receiving objects: 100% (15006/15006), 85.77 MiB | 11.90 MiB/s, done.\n",
            "Checking out files: 100% (15004/15004), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TMNXu_j2QNK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "4427c427-2fc3-4b0b-8b7b-b01f6eaf1e16"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OTGbHHL1aAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CHARS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'K', 'M',\n",
        "         'N', 'P', 'R', 'T', 'U', 'V', 'W', 'X', 'Y']\n",
        "\n",
        "ONE_HOT = torch.eye(len(CHARS))\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uw4Q38y1aAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, folder, img_list, transform=None):\n",
        "        self.folder = folder\n",
        "        self.im_list = img_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.im_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.im_list[idx][:4]\n",
        "        path = os.path.join(self.folder, self.im_list[idx])\n",
        "        im = Image.open(path)\n",
        "        if im.mode != 'RGB':\n",
        "            im = im.convert('RGB')\n",
        "        sample = {'image': im, 'label': label}\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX7aKNCj1aAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2OneHot(object):\n",
        "    def __call__(self, sample):\n",
        "        labels = list()\n",
        "        for c in sample['label']:\n",
        "            idx = CHARS.index(c)\n",
        "            labels.append(ONE_HOT[idx])\n",
        "        sample['label'] = torch.cat(labels)\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HpVeyM_1aAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImgToTensor(object):\n",
        "    def __call__(self, sample):\n",
        "        np_img = np.asarray(sample['image'])\n",
        "        image = np_img.transpose((2, 0, 1))  # H x W x C  -->  C x H x W\n",
        "        sample['image'] = torch.from_numpy(image).float()\n",
        "        return sample\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJOP3HdP1aAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Normalize(transforms.Normalize):\n",
        "    def __call__(self, sample):\n",
        "        tensor = sample['image']\n",
        "        sample['image'] = functional.normalize(\n",
        "            tensor, self.mean, self.std, self.inplace)\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "327Wo0RH1aAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ToGPU(object):\n",
        "    def __call__(self, sample):\n",
        "        sample['image'] = sample['image'].to(DEVICE)\n",
        "        sample['label'] = sample['label'].float().to(DEVICE)\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WyngNNy1aAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(batch_size=4, max_m=-1, split_rate=0.2, gpu=True):\n",
        "    # list images\n",
        "    folder = 'datasets1/'\n",
        "    imgs = [i for i in os.listdir(folder) if i.endswith('jpg')]\n",
        "    if not imgs:\n",
        "        raise Exception('Empty folder!')\n",
        "    random.seed(1)\n",
        "    random.shuffle(imgs)\n",
        "    point = int(split_rate * len(imgs))\n",
        "    train_imgs = imgs[point:][:max_m]\n",
        "    valid_imgs = imgs[:point][:max_m]\n",
        "\n",
        "    # initialize transform\n",
        "    chains = [Word2OneHot(),\n",
        "              ImgToTensor(),\n",
        "              Normalize([127.5, 127.5, 127.5], [128, 128, 128])]\n",
        "    if gpu:\n",
        "        chains.append(ToGPU())\n",
        "    transform = transforms.Compose(chains)\n",
        "\n",
        "    # load data\n",
        "    train_ds = ImageDataset(folder, train_imgs, transform=transform)\n",
        "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    valid_ds = ImageDataset(folder, valid_imgs, transform=transform)\n",
        "    valid_dl = DataLoader(valid_ds, batch_size=batch_size)\n",
        "    return train_dl, valid_dl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07rE1uL91aAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img):\n",
        "    img = img * 128 + 127.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    npimg = np.transpose(npimg, (1, 2, 0))\n",
        "    im = Image.fromarray(npimg.astype('uint8'))\n",
        "    im.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD311L9y1aA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def human_time(start, end):\n",
        "    s = int(end-start)\n",
        "    if s < 60:\n",
        "        return '{}s'.format(s)\n",
        "    m = s // 60\n",
        "    s = s % 60\n",
        "    if m < 59:\n",
        "        return '{}m {}s'.format(m, s)\n",
        "    h = m // 60\n",
        "    m = m % 60\n",
        "    return '{}h {}m {}s'.format(h, m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZijTd51X1aA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchviz import make_dot\n",
        "import torch.nn.functional as F\n",
        "from timeit import default_timer as timer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGSe5joC1aBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, gpu=False):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 18, 5)  # 18 * 32 * 116\n",
        "        self.pool1 = nn.MaxPool2d(2)  # 18 * 16 * 58\n",
        "        self.conv2 = nn.Conv2d(18, 48, 5)  # 48 * 12 * 54\n",
        "        self.pool2 = nn.MaxPool2d(2)  # 48 * 6 * 27\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(48 * 6 * 27, 360)\n",
        "        self.fc2 = nn.Linear(360, 19 * 4)\n",
        "\n",
        "        if gpu:\n",
        "            self.to(DEVICE)\n",
        "            if str(DEVICE) == 'cpu':\n",
        "                self.device = 'cpu'\n",
        "            else:\n",
        "                self.device = torch.cuda.get_device_name(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 48 * 6 * 27)\n",
        "        x = self.drop(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x).view(-1, 4, 19)\n",
        "        x = F.softmax(x, dim=2)\n",
        "        x = x.view(-1, 4 * 19)\n",
        "        return x\n",
        "\n",
        "    def save(self, name, folder='./models'):\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "        path = os.path.join(folder, name)\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load(self, name, folder='./models'):\n",
        "        path = os.path.join(folder, name)\n",
        "        map_location = 'cpu' if self.device == 'cpu' else 'gpu'\n",
        "        static_dict = torch.load(path, map_location)\n",
        "        self.load_state_dict(static_dict)\n",
        "        self.eval()\n",
        "\n",
        "    def graph(self):\n",
        "        x = torch.rand(1, 3, 36, 120)\n",
        "        y = self(x)\n",
        "        return make_dot(y, params=dict(self.named_parameters()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R42I4mk31aBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_batch(model, loss_func, data, opt=None):\n",
        "    xb, yb = data['image'], data['label']\n",
        "    batch_size = len(xb)\n",
        "    out = model(xb)\n",
        "    loss = loss_func(out, yb)\n",
        "\n",
        "    single_correct, whole_correct = 0, 0\n",
        "    if opt is not None:\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    else:  # calc accuracy\n",
        "        yb = yb.view(-1, 4, 19)\n",
        "        out_matrix = out.view(-1, 4, 19)\n",
        "        _, ans = torch.max(yb, 2)\n",
        "        _, predicted = torch.max(out_matrix, 2)\n",
        "        compare = (predicted == ans)\n",
        "        single_correct = compare.sum().item()\n",
        "        for i in range(batch_size):\n",
        "            if compare[i].sum().item() == 4:\n",
        "                whole_correct += 1\n",
        "        del out_matrix\n",
        "    loss_item = loss.item()\n",
        "    del out\n",
        "    del loss\n",
        "    return loss_item, single_correct, whole_correct, batch_size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8FuIPWl1aBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl, verbose=None):\n",
        "    max_acc = 0\n",
        "    patience_limit = 10\n",
        "    patience = 0\n",
        "    for epoch in range(epochs):\n",
        "        patience += 1\n",
        "        running_loss = 0.0\n",
        "        total_nums = 0\n",
        "        model.train()\n",
        "        for i, data in enumerate(train_dl):\n",
        "            loss, _, _, s = loss_batch(model, loss_func, data, opt)\n",
        "            if isinstance(verbose, int):\n",
        "                running_loss += loss * s\n",
        "                total_nums += s\n",
        "                if i % verbose == verbose - 1:\n",
        "                    ave_loss = running_loss / total_nums\n",
        "                    print('[Epoch {}][Batch {}] got training loss: {:.6f}'\n",
        "                          .format(epoch + 1, i + 1, ave_loss))\n",
        "                    total_nums = 0\n",
        "                    running_loss = 0.0\n",
        "\n",
        "        model.eval()  # validate model, working for drop out layer.\n",
        "        with torch.no_grad():\n",
        "            losses, single, whole, batch_size = zip(\n",
        "                *[loss_batch(model, loss_func, data) for data in valid_dl]\n",
        "            )\n",
        "        total_size = np.sum(batch_size)\n",
        "        val_loss = np.sum(np.multiply(losses, batch_size)) / total_size\n",
        "        single_rate = 100 * np.sum(single) / (total_size * 4)\n",
        "        whole_rate = 100 * np.sum(whole) / total_size\n",
        "        if single_rate > max_acc:\n",
        "            patience = 0\n",
        "            max_acc = single_rate\n",
        "            model.save('pretrained')\n",
        "\n",
        "        print('After epoch {}: \\n'\n",
        "              '\\tLoss: {:.6f}\\n'\n",
        "              '\\tSingle Acc: {:.2f}%\\n'\n",
        "              '\\tWhole Acc: {:.2f}%'\n",
        "              .format(epoch + 1, val_loss, single_rate, whole_rate))\n",
        "        if patience > patience_limit:\n",
        "            print('Early stop at epoch {}'.format(epoch + 1))\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvixJ05o1aBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(use_gpu=True, epochs=50,verbose=500):\n",
        "    train_dl, valid_dl = load_data(batch_size=4, split_rate=0.2, gpu=use_gpu)\n",
        "    model = Net(use_gpu)\n",
        "    opt = optim.Adadelta(model.parameters())\n",
        "    criterion = nn.BCELoss()\n",
        "    start = timer()\n",
        "    fit(epochs, model, criterion, opt, train_dl, valid_dl, verbose)\n",
        "    end = timer()\n",
        "    t = human_time(start, end)\n",
        "    print('Total training time using {}: {}'.format(model.device, t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wzDTS0z1aBM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7531
        },
        "outputId": "e674e9f0-2719-4704-dedb-e0d8615f206a"
      },
      "source": [
        "train()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1][Batch 500] got training loss: 0.203924\n",
            "[Epoch 1][Batch 1000] got training loss: 0.192216\n",
            "[Epoch 1][Batch 1500] got training loss: 0.171199\n",
            "[Epoch 1][Batch 2000] got training loss: 0.145604\n",
            "[Epoch 1][Batch 2500] got training loss: 0.125308\n",
            "[Epoch 1][Batch 3000] got training loss: 0.109188\n",
            "After epoch 1: \n",
            "\tLoss: 0.088903\n",
            "\tSingle Acc: 64.38%\n",
            "\tWhole Acc: 20.57%\n",
            "[Epoch 2][Batch 500] got training loss: 0.089395\n",
            "[Epoch 2][Batch 1000] got training loss: 0.082700\n",
            "[Epoch 2][Batch 1500] got training loss: 0.078190\n",
            "[Epoch 2][Batch 2000] got training loss: 0.073603\n",
            "[Epoch 2][Batch 2500] got training loss: 0.070459\n",
            "[Epoch 2][Batch 3000] got training loss: 0.065678\n",
            "After epoch 2: \n",
            "\tLoss: 0.050220\n",
            "\tSingle Acc: 80.54%\n",
            "\tWhole Acc: 46.32%\n",
            "[Epoch 3][Batch 500] got training loss: 0.052468\n",
            "[Epoch 3][Batch 1000] got training loss: 0.049962\n",
            "[Epoch 3][Batch 1500] got training loss: 0.049482\n",
            "[Epoch 3][Batch 2000] got training loss: 0.048883\n",
            "[Epoch 3][Batch 2500] got training loss: 0.050550\n",
            "[Epoch 3][Batch 3000] got training loss: 0.047788\n",
            "After epoch 3: \n",
            "\tLoss: 0.038070\n",
            "\tSingle Acc: 86.05%\n",
            "\tWhole Acc: 58.65%\n",
            "[Epoch 4][Batch 500] got training loss: 0.034955\n",
            "[Epoch 4][Batch 1000] got training loss: 0.036612\n",
            "[Epoch 4][Batch 1500] got training loss: 0.037372\n",
            "[Epoch 4][Batch 2000] got training loss: 0.035284\n",
            "[Epoch 4][Batch 2500] got training loss: 0.036654\n",
            "[Epoch 4][Batch 3000] got training loss: 0.038937\n",
            "After epoch 4: \n",
            "\tLoss: 0.031791\n",
            "\tSingle Acc: 88.57%\n",
            "\tWhole Acc: 64.82%\n",
            "[Epoch 5][Batch 500] got training loss: 0.027977\n",
            "[Epoch 5][Batch 1000] got training loss: 0.029083\n",
            "[Epoch 5][Batch 1500] got training loss: 0.029523\n",
            "[Epoch 5][Batch 2000] got training loss: 0.030346\n",
            "[Epoch 5][Batch 2500] got training loss: 0.029782\n",
            "[Epoch 5][Batch 3000] got training loss: 0.030001\n",
            "After epoch 5: \n",
            "\tLoss: 0.028656\n",
            "\tSingle Acc: 89.69%\n",
            "\tWhole Acc: 67.59%\n",
            "[Epoch 6][Batch 500] got training loss: 0.021951\n",
            "[Epoch 6][Batch 1000] got training loss: 0.023123\n",
            "[Epoch 6][Batch 1500] got training loss: 0.024390\n",
            "[Epoch 6][Batch 2000] got training loss: 0.025410\n",
            "[Epoch 6][Batch 2500] got training loss: 0.025031\n",
            "[Epoch 6][Batch 3000] got training loss: 0.025279\n",
            "After epoch 6: \n",
            "\tLoss: 0.029273\n",
            "\tSingle Acc: 89.95%\n",
            "\tWhole Acc: 67.56%\n",
            "[Epoch 7][Batch 500] got training loss: 0.019451\n",
            "[Epoch 7][Batch 1000] got training loss: 0.021648\n",
            "[Epoch 7][Batch 1500] got training loss: 0.021422\n",
            "[Epoch 7][Batch 2000] got training loss: 0.022510\n",
            "[Epoch 7][Batch 2500] got training loss: 0.020797\n",
            "[Epoch 7][Batch 3000] got training loss: 0.022617\n",
            "After epoch 7: \n",
            "\tLoss: 0.026731\n",
            "\tSingle Acc: 91.34%\n",
            "\tWhole Acc: 71.59%\n",
            "[Epoch 8][Batch 500] got training loss: 0.016335\n",
            "[Epoch 8][Batch 1000] got training loss: 0.017265\n",
            "[Epoch 8][Batch 1500] got training loss: 0.017742\n",
            "[Epoch 8][Batch 2000] got training loss: 0.019293\n",
            "[Epoch 8][Batch 2500] got training loss: 0.020596\n",
            "[Epoch 8][Batch 3000] got training loss: 0.021110\n",
            "After epoch 8: \n",
            "\tLoss: 0.028940\n",
            "\tSingle Acc: 91.21%\n",
            "\tWhole Acc: 71.66%\n",
            "[Epoch 9][Batch 500] got training loss: 0.015987\n",
            "[Epoch 9][Batch 1000] got training loss: 0.016600\n",
            "[Epoch 9][Batch 1500] got training loss: 0.016574\n",
            "[Epoch 9][Batch 2000] got training loss: 0.018305\n",
            "[Epoch 9][Batch 2500] got training loss: 0.018468\n",
            "[Epoch 9][Batch 3000] got training loss: 0.018652\n",
            "After epoch 9: \n",
            "\tLoss: 0.027777\n",
            "\tSingle Acc: 91.30%\n",
            "\tWhole Acc: 71.26%\n",
            "[Epoch 10][Batch 500] got training loss: 0.013819\n",
            "[Epoch 10][Batch 1000] got training loss: 0.015079\n",
            "[Epoch 10][Batch 1500] got training loss: 0.015374\n",
            "[Epoch 10][Batch 2000] got training loss: 0.017044\n",
            "[Epoch 10][Batch 2500] got training loss: 0.015255\n",
            "[Epoch 10][Batch 3000] got training loss: 0.016634\n",
            "After epoch 10: \n",
            "\tLoss: 0.027958\n",
            "\tSingle Acc: 91.71%\n",
            "\tWhole Acc: 73.32%\n",
            "[Epoch 11][Batch 500] got training loss: 0.012771\n",
            "[Epoch 11][Batch 1000] got training loss: 0.012732\n",
            "[Epoch 11][Batch 1500] got training loss: 0.017702\n",
            "[Epoch 11][Batch 2000] got training loss: 0.013653\n",
            "[Epoch 11][Batch 2500] got training loss: 0.014950\n",
            "[Epoch 11][Batch 3000] got training loss: 0.015410\n",
            "After epoch 11: \n",
            "\tLoss: 0.029456\n",
            "\tSingle Acc: 91.22%\n",
            "\tWhole Acc: 71.22%\n",
            "[Epoch 12][Batch 500] got training loss: 0.012600\n",
            "[Epoch 12][Batch 1000] got training loss: 0.013149\n",
            "[Epoch 12][Batch 1500] got training loss: 0.014432\n",
            "[Epoch 12][Batch 2000] got training loss: 0.014697\n",
            "[Epoch 12][Batch 2500] got training loss: 0.013875\n",
            "[Epoch 12][Batch 3000] got training loss: 0.014737\n",
            "After epoch 12: \n",
            "\tLoss: 0.031032\n",
            "\tSingle Acc: 91.12%\n",
            "\tWhole Acc: 70.79%\n",
            "[Epoch 13][Batch 500] got training loss: 0.011013\n",
            "[Epoch 13][Batch 1000] got training loss: 0.012272\n",
            "[Epoch 13][Batch 1500] got training loss: 0.014369\n",
            "[Epoch 13][Batch 2000] got training loss: 0.013485\n",
            "[Epoch 13][Batch 2500] got training loss: 0.013033\n",
            "[Epoch 13][Batch 3000] got training loss: 0.014322\n",
            "After epoch 13: \n",
            "\tLoss: 0.026581\n",
            "\tSingle Acc: 91.41%\n",
            "\tWhole Acc: 71.99%\n",
            "[Epoch 14][Batch 500] got training loss: 0.010429\n",
            "[Epoch 14][Batch 1000] got training loss: 0.013074\n",
            "[Epoch 14][Batch 1500] got training loss: 0.012032\n",
            "[Epoch 14][Batch 2000] got training loss: 0.012664\n",
            "[Epoch 14][Batch 2500] got training loss: 0.012345\n",
            "[Epoch 14][Batch 3000] got training loss: 0.013927\n",
            "After epoch 14: \n",
            "\tLoss: 0.035268\n",
            "\tSingle Acc: 89.86%\n",
            "\tWhole Acc: 67.82%\n",
            "[Epoch 15][Batch 500] got training loss: 0.011273\n",
            "[Epoch 15][Batch 1000] got training loss: 0.011886\n",
            "[Epoch 15][Batch 1500] got training loss: 0.012245\n",
            "[Epoch 15][Batch 2000] got training loss: 0.012202\n",
            "[Epoch 15][Batch 2500] got training loss: 0.011436\n",
            "[Epoch 15][Batch 3000] got training loss: 0.013428\n",
            "After epoch 15: \n",
            "\tLoss: 0.026821\n",
            "\tSingle Acc: 92.56%\n",
            "\tWhole Acc: 74.92%\n",
            "[Epoch 16][Batch 500] got training loss: 0.010020\n",
            "[Epoch 16][Batch 1000] got training loss: 0.012154\n",
            "[Epoch 16][Batch 1500] got training loss: 0.012007\n",
            "[Epoch 16][Batch 2000] got training loss: 0.012141\n",
            "[Epoch 16][Batch 2500] got training loss: 0.012486\n",
            "[Epoch 16][Batch 3000] got training loss: 0.012638\n",
            "After epoch 16: \n",
            "\tLoss: 0.028727\n",
            "\tSingle Acc: 92.42%\n",
            "\tWhole Acc: 74.76%\n",
            "[Epoch 17][Batch 500] got training loss: 0.008701\n",
            "[Epoch 17][Batch 1000] got training loss: 0.012352\n",
            "[Epoch 17][Batch 1500] got training loss: 0.012183\n",
            "[Epoch 17][Batch 2000] got training loss: 0.011703\n",
            "[Epoch 17][Batch 2500] got training loss: 0.011052\n",
            "[Epoch 17][Batch 3000] got training loss: 0.011767\n",
            "After epoch 17: \n",
            "\tLoss: 0.027612\n",
            "\tSingle Acc: 92.91%\n",
            "\tWhole Acc: 76.39%\n",
            "[Epoch 18][Batch 500] got training loss: 0.010140\n",
            "[Epoch 18][Batch 1000] got training loss: 0.010176\n",
            "[Epoch 18][Batch 1500] got training loss: 0.009834\n",
            "[Epoch 18][Batch 2000] got training loss: 0.011820\n",
            "[Epoch 18][Batch 2500] got training loss: 0.011765\n",
            "[Epoch 18][Batch 3000] got training loss: 0.010016\n",
            "After epoch 18: \n",
            "\tLoss: 0.028877\n",
            "\tSingle Acc: 93.07%\n",
            "\tWhole Acc: 76.56%\n",
            "[Epoch 19][Batch 500] got training loss: 0.009263\n",
            "[Epoch 19][Batch 1000] got training loss: 0.010146\n",
            "[Epoch 19][Batch 1500] got training loss: 0.011401\n",
            "[Epoch 19][Batch 2000] got training loss: 0.009298\n",
            "[Epoch 19][Batch 2500] got training loss: 0.010540\n",
            "[Epoch 19][Batch 3000] got training loss: 0.010571\n",
            "After epoch 19: \n",
            "\tLoss: 0.027843\n",
            "\tSingle Acc: 93.10%\n",
            "\tWhole Acc: 77.13%\n",
            "[Epoch 20][Batch 500] got training loss: 0.009250\n",
            "[Epoch 20][Batch 1000] got training loss: 0.011954\n",
            "[Epoch 20][Batch 1500] got training loss: 0.010271\n",
            "[Epoch 20][Batch 2000] got training loss: 0.009961\n",
            "[Epoch 20][Batch 2500] got training loss: 0.011695\n",
            "[Epoch 20][Batch 3000] got training loss: 0.010911\n",
            "After epoch 20: \n",
            "\tLoss: 0.029864\n",
            "\tSingle Acc: 92.67%\n",
            "\tWhole Acc: 75.43%\n",
            "[Epoch 21][Batch 500] got training loss: 0.009463\n",
            "[Epoch 21][Batch 1000] got training loss: 0.008774\n",
            "[Epoch 21][Batch 1500] got training loss: 0.010594\n",
            "[Epoch 21][Batch 2000] got training loss: 0.010621\n",
            "[Epoch 21][Batch 2500] got training loss: 0.010012\n",
            "[Epoch 21][Batch 3000] got training loss: 0.010510\n",
            "After epoch 21: \n",
            "\tLoss: 0.026485\n",
            "\tSingle Acc: 93.31%\n",
            "\tWhole Acc: 76.89%\n",
            "[Epoch 22][Batch 500] got training loss: 0.007039\n",
            "[Epoch 22][Batch 1000] got training loss: 0.010591\n",
            "[Epoch 22][Batch 1500] got training loss: 0.009148\n",
            "[Epoch 22][Batch 2000] got training loss: 0.010371\n",
            "[Epoch 22][Batch 2500] got training loss: 0.010047\n",
            "[Epoch 22][Batch 3000] got training loss: 0.008602\n",
            "After epoch 22: \n",
            "\tLoss: 0.030231\n",
            "\tSingle Acc: 93.37%\n",
            "\tWhole Acc: 77.43%\n",
            "[Epoch 23][Batch 500] got training loss: 0.009197\n",
            "[Epoch 23][Batch 1000] got training loss: 0.008105\n",
            "[Epoch 23][Batch 1500] got training loss: 0.010392\n",
            "[Epoch 23][Batch 2000] got training loss: 0.009393\n",
            "[Epoch 23][Batch 2500] got training loss: 0.008634\n",
            "[Epoch 23][Batch 3000] got training loss: 0.009279\n",
            "After epoch 23: \n",
            "\tLoss: 0.028293\n",
            "\tSingle Acc: 92.47%\n",
            "\tWhole Acc: 74.96%\n",
            "[Epoch 24][Batch 500] got training loss: 0.008889\n",
            "[Epoch 24][Batch 1000] got training loss: 0.009775\n",
            "[Epoch 24][Batch 1500] got training loss: 0.007414\n",
            "[Epoch 24][Batch 2000] got training loss: 0.009239\n",
            "[Epoch 24][Batch 2500] got training loss: 0.009436\n",
            "[Epoch 24][Batch 3000] got training loss: 0.008614\n",
            "After epoch 24: \n",
            "\tLoss: 0.029298\n",
            "\tSingle Acc: 93.15%\n",
            "\tWhole Acc: 76.89%\n",
            "[Epoch 25][Batch 500] got training loss: 0.008370\n",
            "[Epoch 25][Batch 1000] got training loss: 0.008491\n",
            "[Epoch 25][Batch 1500] got training loss: 0.007568\n",
            "[Epoch 25][Batch 2000] got training loss: 0.007475\n",
            "[Epoch 25][Batch 2500] got training loss: 0.009621\n",
            "[Epoch 25][Batch 3000] got training loss: 0.010136\n",
            "After epoch 25: \n",
            "\tLoss: 0.027895\n",
            "\tSingle Acc: 93.02%\n",
            "\tWhole Acc: 76.73%\n",
            "[Epoch 26][Batch 500] got training loss: 0.007983\n",
            "[Epoch 26][Batch 1000] got training loss: 0.007429\n",
            "[Epoch 26][Batch 1500] got training loss: 0.007038\n",
            "[Epoch 26][Batch 2000] got training loss: 0.007775\n",
            "[Epoch 26][Batch 2500] got training loss: 0.007826\n",
            "[Epoch 26][Batch 3000] got training loss: 0.007445\n",
            "After epoch 26: \n",
            "\tLoss: 0.031914\n",
            "\tSingle Acc: 93.28%\n",
            "\tWhole Acc: 77.13%\n",
            "[Epoch 27][Batch 500] got training loss: 0.007443\n",
            "[Epoch 27][Batch 1000] got training loss: 0.006485\n",
            "[Epoch 27][Batch 1500] got training loss: 0.007783\n",
            "[Epoch 27][Batch 2000] got training loss: 0.007596\n",
            "[Epoch 27][Batch 2500] got training loss: 0.008436\n",
            "[Epoch 27][Batch 3000] got training loss: 0.008365\n",
            "After epoch 27: \n",
            "\tLoss: 0.031167\n",
            "\tSingle Acc: 93.26%\n",
            "\tWhole Acc: 77.46%\n",
            "[Epoch 28][Batch 500] got training loss: 0.009282\n",
            "[Epoch 28][Batch 1000] got training loss: 0.007705\n",
            "[Epoch 28][Batch 1500] got training loss: 0.006838\n",
            "[Epoch 28][Batch 2000] got training loss: 0.008676\n",
            "[Epoch 28][Batch 2500] got training loss: 0.008000\n",
            "[Epoch 28][Batch 3000] got training loss: 0.009083\n",
            "After epoch 28: \n",
            "\tLoss: 0.034917\n",
            "\tSingle Acc: 92.69%\n",
            "\tWhole Acc: 75.89%\n",
            "[Epoch 29][Batch 500] got training loss: 0.006188\n",
            "[Epoch 29][Batch 1000] got training loss: 0.007174\n",
            "[Epoch 29][Batch 1500] got training loss: 0.007284\n",
            "[Epoch 29][Batch 2000] got training loss: 0.007679\n",
            "[Epoch 29][Batch 2500] got training loss: 0.008698\n",
            "[Epoch 29][Batch 3000] got training loss: 0.008908\n",
            "After epoch 29: \n",
            "\tLoss: 0.033216\n",
            "\tSingle Acc: 92.67%\n",
            "\tWhole Acc: 75.79%\n",
            "[Epoch 30][Batch 500] got training loss: 0.007189\n",
            "[Epoch 30][Batch 1000] got training loss: 0.008449\n",
            "[Epoch 30][Batch 1500] got training loss: 0.008255\n",
            "[Epoch 30][Batch 2000] got training loss: 0.007304\n",
            "[Epoch 30][Batch 2500] got training loss: 0.008396\n",
            "[Epoch 30][Batch 3000] got training loss: 0.008876\n",
            "After epoch 30: \n",
            "\tLoss: 0.030989\n",
            "\tSingle Acc: 93.16%\n",
            "\tWhole Acc: 76.26%\n",
            "[Epoch 31][Batch 500] got training loss: 0.006242\n",
            "[Epoch 31][Batch 1000] got training loss: 0.006379\n",
            "[Epoch 31][Batch 1500] got training loss: 0.006333\n",
            "[Epoch 31][Batch 2000] got training loss: 0.007721\n",
            "[Epoch 31][Batch 2500] got training loss: 0.008005\n",
            "[Epoch 31][Batch 3000] got training loss: 0.007516\n",
            "After epoch 31: \n",
            "\tLoss: 0.031809\n",
            "\tSingle Acc: 92.66%\n",
            "\tWhole Acc: 75.53%\n",
            "[Epoch 32][Batch 500] got training loss: 0.007135\n",
            "[Epoch 32][Batch 1000] got training loss: 0.005990\n",
            "[Epoch 32][Batch 1500] got training loss: 0.005826\n",
            "[Epoch 32][Batch 2000] got training loss: 0.009565\n",
            "[Epoch 32][Batch 2500] got training loss: 0.006519\n",
            "[Epoch 32][Batch 3000] got training loss: 0.008378\n",
            "After epoch 32: \n",
            "\tLoss: 0.033971\n",
            "\tSingle Acc: 92.52%\n",
            "\tWhole Acc: 75.46%\n",
            "[Epoch 33][Batch 500] got training loss: 0.007178\n",
            "[Epoch 33][Batch 1000] got training loss: 0.007501\n",
            "[Epoch 33][Batch 1500] got training loss: 0.006353\n",
            "[Epoch 33][Batch 2000] got training loss: 0.007462\n",
            "[Epoch 33][Batch 2500] got training loss: 0.008726\n",
            "[Epoch 33][Batch 3000] got training loss: 0.005895\n",
            "After epoch 33: \n",
            "\tLoss: 0.031381\n",
            "\tSingle Acc: 93.82%\n",
            "\tWhole Acc: 79.09%\n",
            "[Epoch 34][Batch 500] got training loss: 0.006248\n",
            "[Epoch 34][Batch 1000] got training loss: 0.006684\n",
            "[Epoch 34][Batch 1500] got training loss: 0.009845\n",
            "[Epoch 34][Batch 2000] got training loss: 0.007892\n",
            "[Epoch 34][Batch 2500] got training loss: 0.006620\n",
            "[Epoch 34][Batch 3000] got training loss: 0.007299\n",
            "After epoch 34: \n",
            "\tLoss: 0.033010\n",
            "\tSingle Acc: 92.97%\n",
            "\tWhole Acc: 76.53%\n",
            "[Epoch 35][Batch 500] got training loss: 0.005975\n",
            "[Epoch 35][Batch 1000] got training loss: 0.006828\n",
            "[Epoch 35][Batch 1500] got training loss: 0.006487\n",
            "[Epoch 35][Batch 2000] got training loss: 0.007651\n",
            "[Epoch 35][Batch 2500] got training loss: 0.006282\n",
            "[Epoch 35][Batch 3000] got training loss: 0.007661\n",
            "After epoch 35: \n",
            "\tLoss: 0.033313\n",
            "\tSingle Acc: 93.59%\n",
            "\tWhole Acc: 78.23%\n",
            "[Epoch 36][Batch 500] got training loss: 0.005036\n",
            "[Epoch 36][Batch 1000] got training loss: 0.005176\n",
            "[Epoch 36][Batch 1500] got training loss: 0.006567\n",
            "[Epoch 36][Batch 2000] got training loss: 0.007779\n",
            "[Epoch 36][Batch 2500] got training loss: 0.006976\n",
            "[Epoch 36][Batch 3000] got training loss: 0.007225\n",
            "After epoch 36: \n",
            "\tLoss: 0.033334\n",
            "\tSingle Acc: 93.38%\n",
            "\tWhole Acc: 77.59%\n",
            "[Epoch 37][Batch 500] got training loss: 0.006542\n",
            "[Epoch 37][Batch 1000] got training loss: 0.006579\n",
            "[Epoch 37][Batch 1500] got training loss: 0.007510\n",
            "[Epoch 37][Batch 2000] got training loss: 0.004873\n",
            "[Epoch 37][Batch 2500] got training loss: 0.006966\n",
            "[Epoch 37][Batch 3000] got training loss: 0.011489\n",
            "After epoch 37: \n",
            "\tLoss: 0.030762\n",
            "\tSingle Acc: 91.88%\n",
            "\tWhole Acc: 72.76%\n",
            "[Epoch 38][Batch 500] got training loss: 0.006005\n",
            "[Epoch 38][Batch 1000] got training loss: 0.007264\n",
            "[Epoch 38][Batch 1500] got training loss: 0.006029\n",
            "[Epoch 38][Batch 2000] got training loss: 0.006307\n",
            "[Epoch 38][Batch 2500] got training loss: 0.005446\n",
            "[Epoch 38][Batch 3000] got training loss: 0.007354\n",
            "After epoch 38: \n",
            "\tLoss: 0.034999\n",
            "\tSingle Acc: 93.46%\n",
            "\tWhole Acc: 77.99%\n",
            "[Epoch 39][Batch 500] got training loss: 0.006134\n",
            "[Epoch 39][Batch 1000] got training loss: 0.004706\n",
            "[Epoch 39][Batch 1500] got training loss: 0.006192\n",
            "[Epoch 39][Batch 2000] got training loss: 0.006022\n",
            "[Epoch 39][Batch 2500] got training loss: 0.006280\n",
            "[Epoch 39][Batch 3000] got training loss: 0.006807\n",
            "After epoch 39: \n",
            "\tLoss: 0.038635\n",
            "\tSingle Acc: 93.56%\n",
            "\tWhole Acc: 77.76%\n",
            "[Epoch 40][Batch 500] got training loss: 0.006462\n",
            "[Epoch 40][Batch 1000] got training loss: 0.007795\n",
            "[Epoch 40][Batch 1500] got training loss: 0.006615\n",
            "[Epoch 40][Batch 2000] got training loss: 0.005930\n",
            "[Epoch 40][Batch 2500] got training loss: 0.006053\n",
            "[Epoch 40][Batch 3000] got training loss: 0.006649\n",
            "After epoch 40: \n",
            "\tLoss: 0.038627\n",
            "\tSingle Acc: 93.32%\n",
            "\tWhole Acc: 77.73%\n",
            "[Epoch 41][Batch 500] got training loss: 0.005942\n",
            "[Epoch 41][Batch 1000] got training loss: 0.006816\n",
            "[Epoch 41][Batch 1500] got training loss: 0.008995\n",
            "[Epoch 41][Batch 2000] got training loss: 0.004934\n",
            "[Epoch 41][Batch 2500] got training loss: 0.005299\n",
            "[Epoch 41][Batch 3000] got training loss: 0.006665\n",
            "After epoch 41: \n",
            "\tLoss: 0.031898\n",
            "\tSingle Acc: 92.84%\n",
            "\tWhole Acc: 75.86%\n",
            "[Epoch 42][Batch 500] got training loss: 0.006650\n",
            "[Epoch 42][Batch 1000] got training loss: 0.007577\n",
            "[Epoch 42][Batch 1500] got training loss: 0.005908\n",
            "[Epoch 42][Batch 2000] got training loss: 0.006869\n",
            "[Epoch 42][Batch 2500] got training loss: 0.005908\n",
            "[Epoch 42][Batch 3000] got training loss: 0.008080\n",
            "After epoch 42: \n",
            "\tLoss: 0.037685\n",
            "\tSingle Acc: 93.26%\n",
            "\tWhole Acc: 77.26%\n",
            "[Epoch 43][Batch 500] got training loss: 0.005881\n",
            "[Epoch 43][Batch 1000] got training loss: 0.005347\n",
            "[Epoch 43][Batch 1500] got training loss: 0.004953\n",
            "[Epoch 43][Batch 2000] got training loss: 0.006024\n",
            "[Epoch 43][Batch 2500] got training loss: 0.004638\n",
            "[Epoch 43][Batch 3000] got training loss: 0.006962\n",
            "After epoch 43: \n",
            "\tLoss: 0.031221\n",
            "\tSingle Acc: 93.47%\n",
            "\tWhole Acc: 77.96%\n",
            "[Epoch 44][Batch 500] got training loss: 0.006177\n",
            "[Epoch 44][Batch 1000] got training loss: 0.005419\n",
            "[Epoch 44][Batch 1500] got training loss: 0.004676\n",
            "[Epoch 44][Batch 2000] got training loss: 0.004983\n",
            "[Epoch 44][Batch 2500] got training loss: 0.007220\n",
            "[Epoch 44][Batch 3000] got training loss: 0.005009\n",
            "After epoch 44: \n",
            "\tLoss: 0.034267\n",
            "\tSingle Acc: 93.72%\n",
            "\tWhole Acc: 78.63%\n",
            "Early stop at epoch 44\n",
            "Total training time using Tesla K80: 29m 14s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2ZFFwHq1aBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}